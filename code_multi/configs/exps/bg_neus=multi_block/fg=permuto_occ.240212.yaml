Experiment explain:
- Last update: 2024.02.12
- Representation:
  - rigid fg with gtbox = generative permuto neus
  - nonrigid fg with gtbox = generative dynamic permuto neus
  - bg = multi-block (aka. forest) neus
- Used information:
  - with_gtbox = True
  - with_mask = True
  - with_lidar = True

#------------------------------------------------------------
#------------    Some shortcut configs
#------------------------------------------------------------

device_ids: -1

near: 0.05
far: 300.0

coarse_step_size: 6.0 # 1.5 Typical value: (far-near) / 40
step_size: 0.15 # 0.02 # Typical value: (far-near) / 4000
num_iters: 30000
extend_size: 60.0

upsample_inv_s: 64.0
num_rays_pixel: 8192
num_rays_lidar: 8192

radius_scale_min: 1 # Nearest sampling shell of NeRF++ background (Distant-view model)
radius_scale_max: 1000 # Furthest sampling shell of NeRF++ background (Distant-view model)
distant_interval_type: inverse_proportional
distant_mode: fixed_cuboid_shells
distant_nsample: 64

occ_thre: 0.3
occ_inv_s: 256.0
occ_iter: 16

sdf_scale: 1.0 # The real-world length represented by one unit of SDF

rgb_fn: l1
rgb_fn_param: {}

lidar_fn: l1
lidar_fn_param: {}
w_lidar: 0.02
w_los: 0.1
# eps_los: anneal_1.5_0.75_0.5

w_mask: 0.25

num_uniform: ${eval:"2**16"}

w_eikonal: 0.001
on_render_ratio: 0.2
on_occ_ratio: 1.0
on_render_type: both
safe_mse: true
errlim: 5
w_sparsity: 3.0e-3
sparsity_enable_after: 0

clbeta: 10.0
clw: 0.2
clearance_sdf: ${eval:"0.5/${sdf_scale}"} # 0.02 * (sdf_scale=25) = 0.5m

warmup_steps: 2000
min_factor: 0.2

veh_lr: 1.0e-2
veh_wei: 1.0e-3
veh_wei_on_render_ratio: 0.02
veh_dtype: half
veh_log2_hashmap_size: 20
veh_n_levels: 16
veh_n_feats: 2
veh_latent_dim: 4
veh_refine_lr: 0.001 # 0.01 is too large

ped_lr: 1.0e-2
ped_latent_dim: 2
ped_nc: 16
ped_log2_hashmap_size: 20
ped_n_levels: 12
ped_n_feats: 4
ped_step: 0.05
ped_bounding_size: 2.4
ped_lnini: 0.1
ped_factor: 10.0
ped_accel_ema: 0.97
ped_fine_res: 1200.0
ped_errmap_focus: 0.2

ins_latent_init_method: zero
frame_latent_init_method: linspace
frame_latent_learnable: false

lnini: 0.1

use_estimate_alpha: false

geo_init_method: pretrain_after_zero_out # pretrain

image_embedding_dim: 4

# camera_list: [camera_FRONT_LEFT, camera_FRONT, camera_FRONT_RIGHT]
camera_list: [camera_SIDE_LEFT, camera_FRONT_LEFT, camera_FRONT, camera_FRONT_RIGHT, camera_SIDE_RIGHT]
lidar_list: [lidar_TOP, lidar_FRONT, lidar_REAR, lidar_SIDE_LEFT, lidar_SIDE_RIGHT]
lidar_weight: [0.4,0.1,0.1,0.1,0.1] # Will be normalized when using

exp_dir: ./logs/neuralsim.multi_block/exp_multi_waymo_seg767010_ext${extend_size}_lidarhalf_float_lotforestv3_30k_fix3

dataset_cfg: 
  target: dataio.autonomous_driving.WaymoDataset
  param:
    # root: /nvme/guojianfei/waymo/processed/
    root: /data1/waymo/processed/
    # root: /home/ventus/datasets/waymo/processed/
    # root: ./data/waymo/processed/
    rgb_dirname: images
    lidar_dirname: lidars

    mask_dirname: masks
    # mask_dirname: masks_vit_adapter
    # mask_dirname: masks_vit_adapter
    # mask_taxonomy: ade20k

scenebank_cfg:
  # NOTE: scene_id[,start_frame[,n_frames]]
  scenarios:
    - segment-7670103006580549715_360_000_380_000_with_camera_labels, 15
  observer_cfgs: 
    Camera:
      list: ${camera_list}
    RaysLidar:
      list: ${lidar_list}
  object_cfgs:
    Vehicle:
      dynamic_only: true
    Pedestrian:
      dynamic_only: true
    Cyclist:
      dynamic_only: true
  load_class_names: [Street, Vehicle, Pedestrian, Cyclist]
  no_objects: false # Set to true to skip loading foreground objects into scene graph
  align_orientation: false
  consider_distortion: true
  scene_graph_has_ego_car: true

assetbank_cfg:
  Street:
    model_class: app.models.large.LoTDForestNeuSStreet
    model_params:
      dtype: half
      var_ctrl_cfg:
        ln_inv_s_init: 0.3
        ln_inv_s_factor: 10.0
      cos_anneal_cfg: null
      surface_cfg:
        sdf_scale: ${sdf_scale}
        # clip_level_grad_ema_factor: 2.0
        encoding_cfg:
          lotd_cfg:
            #------------- LoT v3
            lod_res:     [16,    32,    64,    128, 256]
            lod_n_feats: [8,     4,     2,     8,   4]
            lod_types:   [Dense, Dense, Dense, VM,  VM]
            #------------- hash v4.2
            # lod_res:     [20, 29, 40, 55, 76, 105, 146, 202, 279, 385, 533, 737, 1018]
            # lod_n_feats: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
            # lod_types:   ['Dense', 'Dense', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash', 'Hash']
            # hashmap_size: 65536
          param_init_cfg:
            type: uniform_to_type
            bound: 1.0e-4
        decoder_cfg: 
          # type: linear
          type: mlp
          D: 1
          W: 64
        n_extra_feat_from_output: 0
        radius_init: 0.5
        geo_init_method: pretrain
      radiance_cfg:
        use_pos: true
        use_view_dirs: true
        dir_embed_cfg: 
          type: spherical
          degree: 4
        D: 2
        W: 64
        n_appear_embedding: ${image_embedding_dim}
      use_tcnn_backend: false
      accel_cfg:
        type: occ_grid_forest
        resolution: [16,16,16]
        occ_val_fn_cfg:
          type: sdf
          # inv_s: 64.0 # ~> +- 0.1 sdf @ 0.01 thre;  +- 0.027 sdf @ 0.5 thre
          inv_s: ${occ_inv_s}
        occ_thre: ${occ_thre}
        ema_decay: 0.95
        init_cfg:
          mode: from_net
          num_steps: 4
          num_pts: ${eval:"2**22"}
        update_from_net_cfg:
          num_steps: 4
          num_pts: ${eval:"2**22"}
        update_from_samples_cfg: {}
        n_steps_between_update: 16
        n_steps_warmup: 256
      ray_query_cfg:
        # query_mode: inblock_march_occ_multi_upsample
        query_mode: inblock_march_occ_multi_upsample_compressed
        query_param:
          nablas_has_grad: true
          should_sample_coarse: true
          coarse_step_cfg:
            step_mode: linear
            step_size: ${coarse_step_size} # Typical value: (far-near) / 40
          march_cfg:
            step_size: ${step_size} # Typical value: (far-near) / 4000
            max_steps: 8192
          num_fine: 8
          upsample_inv_s: ${upsample_inv_s}
          upsample_inv_s_factors: [1, 4, 16]
    asset_params:
      initialize_cfg: 
        target_shape: road_surface
        lr: 1.0e-3
        num_iters: 2000
        num_samples: 300000
        w_eikonal: 3.0e-3
        floor_dim: z
        floor_up_sign: 1
        ego_height: 0.5
      populate_cfg: 
        mode: inside_frustum
        extend_size: ${extend_size}
        block_cfgs:
          split_level: 3
          should_force_to_power_of_two: true
          overlap: 0
      training_cfg:
        lr: 1.0e-2
        eps: 1.0e-15
        betas: [0.9, 0.99]
        invs_betas: [0.9, 0.999]
        scheduler: ${training.scheduler}
  Distant:
    model_class: app.models.single.LoTDNeRFDistant
    model_params:
      dtype: half
      encoding_cfg:
        input_ch: 4
        lotd_use_cuboid: true
        lotd_auto_compute_cfg:
          type: ngp4d
          target_num_params: ${eval:"16*(2**20)"} # 16 Mi params
          min_res_xyz: 16
          min_res_w: 4
          n_feats: 2
          log2_hashmap_size: 19
          per_level_scale: 1.382
        param_init_cfg:
          type: uniform_to_type
          bound: 1.0e-4
        # anneal_cfg:
        #   type: hardmask
        #   start_it: ${start_it}
        #   start_level: ${bg_start_level} # (need to be small: so the training is stable; not too small, so there's still valid initialize pretraining.)
        #   stop_it: ${stop_it} # Not for too much iters; should end very soon to not hinder quality
      extra_pos_embed_cfg:
        type: identity
      density_decoder_cfg: 
        type: mlp
        D: 1
        W: 64
        output_activation: softplus
      radiance_decoder_cfg:
        use_pos: false
        # pos_embed_cfg:
        #   type: identity
        use_view_dirs: false
        # dir_embed_cfg:
        #   type: spherical
        #   degree: 4
        use_nablas: false
        D: 2
        W: 64
        n_appear_embedding: ${image_embedding_dim}
      n_extra_feat_from_output: 0
      use_tcnn_backend: false
      include_inf_distance: false # !!! has sky
      radius_scale_min: ${radius_scale_min}
      radius_scale_max: ${radius_scale_max}
      ray_query_cfg:
        query_mode: march
        query_param:
          march_cfg:
            interval_type: ${distant_interval_type}
            sample_mode: ${distant_mode}
            max_steps: ${distant_nsample}
    asset_params:
      populate_cfg:
        cr_obj_classname: Street
      training_cfg:
        lr: 1.0e-2
        eps: 1.0e-15
        betas: [0.9, 0.99]
        scheduler: ${training.scheduler}
  Sky:
    model_class: app.models.env.SimpleSky
    model_params: 
      dir_embed_cfg:
        type: sinusoidal
        n_frequencies: 10
        use_tcnn_backend: false
      D: 2
      W: 256
      use_tcnn_backend: false
      n_appear_embedding: ${image_embedding_dim}
    asset_params:
      training_cfg:
        lr: 1.0e-3
        scheduler: ${training.scheduler}
  Vehicle:
    model_class: app.models.shared.AD_GenerativePermutoConcatNeuSObj
    model_params:
      latents_cfg:
        z_ins:
          dim: ${veh_latent_dim}
          weight_init: ${ins_latent_init_method}
      dtype: ${veh_dtype}
      var_ctrl_cfg:
        ln_inv_s_init: ${lnini}
        ln_inv_s_factor: 10.0
      surface_cfg:
        bounding_size: 1.4
        encoding_cfg:
          permuto_auto_compute_cfg:
            type: multi_res
            coarsest_res: 16.0
            finest_res: 2000.0
            n_levels: ${veh_n_levels}
            n_feats: ${veh_n_feats}
            log2_hashmap_size: ${veh_log2_hashmap_size}
            apply_random_shifts_per_level: true
        decoder_cfg:
          type: mlp
          D: 1
          W: 64
        geo_init_method: pretrain
      radiance_cfg:
        pos_embed_cfg:
          type: identity
        use_view_dirs: true
        dir_embed_cfg:
          type: spherical
          degree: 4
        D: 2
        W: 64
        skips: []
      accel_cfg:
        type: occ_grid_batched_ema
        resolution: [32,32,32]
        occ_val_fn_cfg:
          type: sdf
          inv_s: 256.0
        occ_thre: 0.3
        ema_decay: 0.95
        init_cfg:
          mode: from_net
          num_steps: 4
          num_pts: ${eval:"2**20"}
        update_from_net_cfg:
          num_steps: 4
          num_pts: ${eval:"2**20"}
        update_from_samples_cfg: {}
        n_steps_between_update: 16
        n_steps_warmup: 256
      ray_query_cfg:
        query_mode: march_occ_multi_upsample_compressed
        query_param:
          nablas_has_grad: true
          num_coarse: 32
          num_fine: 16
          upsample_inv_s: 64.0
          upsample_inv_s_factors: [1, 4]
          coarse_step_cfg:
            step_mode: linear
          march_cfg:
            step_size: 0.05
            max_steps: 256
    asset_params:
      initialize_cfg:
        num_iters: 3000
        lr: 1.0e-3
        num_points: 5000
        resample_z: true
        batch_size: 16
      training_cfg:
        lr: ${veh_lr}
        eps: 1.0e-15
        betas: [0.9, 0.99]
        invs_betas: [0.9, 0.999]
        scheduler: ${training.scheduler}
  Pedestrian:
    model_class: app.models.shared.AD_Dynamic_GenerativePermutoConcatNeuSObj_Decomp
    model_params:
      latents_cfg:
        z_ins:
          dim: ${ped_latent_dim}
          weight_init: ${ins_latent_init_method}
        z_time:
          dim: 1
          learnable: ${frame_latent_learnable}
          weight_init: 
            type: ${frame_latent_init_method}
            start: -10.0
            end: 10.0
      dtype: ${veh_dtype}
      var_ctrl_cfg:
        ln_inv_s_init: ${ped_lnini}
        ln_inv_s_factor: ${ped_factor}
      surface_cfg:
        bounding_size: ${ped_bounding_size}
        encoding_cfg:
          permuto_auto_compute_cfg:
            type: multi_res
            coarsest_res: 4.0
            finest_res: ${ped_fine_res}
            n_levels: ${ped_n_levels}
            n_feats: ${ped_n_feats}
            log2_hashmap_size: ${ped_log2_hashmap_size}
            apply_random_shifts_per_level: true
        decoder_cfg:
          type: mlp
          D: 1
          W: 64
        geo_init_method: pretrain
      radiance_cfg:
        pos_embed_cfg:
          type: identity
        use_view_dirs: false
        # use_view_dirs: true
        # dir_embed_cfg:
        #   type: spherical
        #   degree: 4
        D: 2
        W: 64
        skips: []
      accel_cfg:
        type: occ_grid_batched_dynamic_ema
        # resolution: [32,32,32]
        occ_val_fn_cfg:
          type: sdf
          inv_s: 256.0
        occ_thre: 0.3
        ema_decay: ${ped_accel_ema}
        init_cfg:
          mode: from_net
          num_steps: 4
          num_pts: ${eval:"2**22"}
        update_from_net_cfg:
          num_steps: 4
          num_pts: ${eval:"2**20"}
        update_from_samples_cfg: {}
        n_steps_between_update: 16
        n_steps_warmup: 256
      ray_query_cfg:
        query_mode: march_occ_multi_upsample_compressed
        query_param:
          nablas_has_grad: true
          num_coarse: ${ped_nc}
          num_fine: 16
          upsample_inv_s: 64.0
          upsample_inv_s_factors: [1, 4]
          coarse_step_cfg:
            step_mode: linear
          march_cfg:
            step_size: ${ped_step}
            max_steps: 512
    asset_params:
      populate_cfg:
        accel_n_jump_frames: 2
        accel_use_avg_resolution: true
        accel_vox_size: 0.2
      initialize_cfg:
        num_iters: 3000
        lr: 1.0e-3
        num_points: 5000
        resample_z: true
        batch_size: 16
      training_cfg:
        lr: ${ped_lr}
        eps: 1.0e-15
        betas: [0.9, 0.99]
        invs_betas: [0.9, 0.999]
        scheduler: ${training.scheduler}
  Cyclist: ${.Pedestrian}
  ImageEmbeddings:
    model_class: app.models.scene.ImageEmbeddings
    model_params:
      dims: ${image_embedding_dim}
      weight_init: uniform
      weight_init_std: 1.0e-4
    asset_params:
      training_cfg:
        lr: 2.0e-2
        scheduler: ${training.scheduler}
  #--- Pose refine related
  LearnableParams:
    model_class: app.models.scene.LearnableParams
    model_params:
      refine_ego_motion:
        # node_id: ego_car
        class_name: Camera
      refine_other_motion: 
        class_name: [ Vehicle ]
      refine_camera_intr: null
      refine_camera_extr: null
      enable_after: 500
    asset_params:
      training_cfg:
        ego_motion:
          lr: 0.001
          alpha_lr_rotation: 0.05
        other_motion:
          lr: ${veh_refine_lr}
          alpha_lr_rotation: 0.05
        scheduler: ${training.scheduler}

renderer:
  common:
    with_env: true # !!! has sky
    with_rgb: true
    with_normal: true
    near: ${near} # NOTE: Critical to scene scale!
    far: ${far}
  train:
    depth_use_normalized_vw: false # For meaningful depth supervision (if any)
    perturb: true
  val:
    depth_use_normalized_vw: true # For correct depth rendering
    perturb: false
    rayschunk: 4096

training:
  #---------- Dataset and sampling
  dataloader:
    preload: true
    preload_on_gpu: false
    tags:
      camera:
        downscale: 1
        list: ${camera_list}
      image_occupancy_mask: {}
      image_human_mask: {}
      image_ignore_mask:
        ignore_not_occupied: false
        ignore_dynamic: false
        ignore_human: true
      lidar:
        list: ${lidar_list}
        multi_lidar_merge: true
        filter_when_preload: false # Significantly slows up preload if true
        filter_kwargs:
          filter_in_cams: true
          filter_out_objs: false # !!! We want this
          filter_valid: true
    pixel_dataset:
      #---------- Frame and pixel dataloader
      # joint: false
      # equal_mode: ray_batch
      # num_rays: ${num_rays_pixel}
      # frame_sample_mode: uniform
      # pixel_sample_mode: error_map

      #---------- Joint frame-pixel dataloader
      joint: true
      equal_mode: ray_batch
      num_rays: ${num_rays_pixel}
    lidar_dataset:
      equal_mode: ray_batch
      num_rays: ${num_rays_lidar}
      frame_sample_mode: uniform
      lidar_sample_mode: merged_weighted
      multi_lidar_weight: ${lidar_weight} # Will be normalized when used
  val_dataloader:
    preload: false
    tags:
      camera:
        downscale: 4
        list: ${camera_list}
      image_occupancy_mask: {}
      image_human_mask: {}
      image_ignore_mask:
        ignore_not_occupied: false
        ignore_dynamic: false
        ignore_human: true
    image_dataset:
      camera_sample_mode: uniform
      frame_sample_mode: uniform

  error_map:
    error_map_hw: [32,64]
    frac_uniform: ${eval:"0.5-${ped_errmap_focus}"}
    frac_on_classnames: ${ped_errmap_focus}
    on_classnames: [ Pedestrian, Cyclist ]
    frac_mask_err: 0
    n_steps_max: 500 # NOTE: The actual effective time of this number now needs to be multiplied by the number of cameras! (Because each iteration samples a camera uniformly at random)

  #---------- Training losses
  uniform_sample: 
    Street: ${num_uniform}
    Vehicle: ${num_uniform}
    Pedestrian: ${num_uniform}
    Cyclist: ${num_uniform}

  losses:
    rgb: 
      fn_type: ${rgb_fn}
      fn_param: ${rgb_fn_param}
      respect_ignore_mask: false # Nothing is ignored for now.
    occupancy_mask:
      w: ${w_mask}
      safe_bce: true
      pred_clip: 0
    lidar:
      discard_outliers: 0
      discard_outliers_median: 100.0
      discard_toofar: 80.0
      depth: 
        w: ${w_lidar}
        fn_type: ${lidar_fn}
        fn_param: ${lidar_fn_param}
      line_of_sight:
        w: ${w_los}
        fn_type: neus_unisim
        fn_param:
          # epsilon: ${eps_los}
          epsilon_anneal:
            type: milestones
            milestones: [5000, 10000]
            vals: [1.5, 0.75, 0.5]
    eikonal:
      safe_mse: ${safe_mse}
      safe_mse_err_limit: ${errlim}
      alpha_reg_zero: 0
      on_occ_ratio: ${on_occ_ratio}
      on_render_type: ${on_render_type}
      on_render_ratio: ${on_render_ratio}
      class_name_cfgs:
        Street:
          w: ${w_eikonal}
        Vehicle:
          w: ${veh_wei}
          on_occ_ratio: 0
          on_render_ratio: ${veh_wei_on_render_ratio}
        Pedestrian:
          w: ${veh_wei}
          on_occ_ratio: 0
          on_render_ratio: ${veh_wei_on_render_ratio}
        Cyclist:
          w: ${veh_wei}
          on_occ_ratio: 0
          on_render_ratio: ${veh_wei_on_render_ratio}
    sparsity:
      enable_after: ${sparsity_enable_after}
      class_name_cfgs:
        Street:
          key: sdf
          w: ${w_sparsity} # 1.0e-3
          # anneal: 
          #   type: milestones
          #   milestones:
          #     - ${eval:"${training.num_iters}/3*2"}
          #     - ${training.num_iters}
          #   vals:
          #     - ${w_sparsity}
          #     - ${eval:"${w_sparsity}/10."}
      fn_type: normalized_logistic_density
      fn_param: 
        inv_scale: 16.0
    clearance:
      class_name_cfgs:
        Street:
          w: ${clw}
          beta: ${clbeta}
          thresh: ${clearance_sdf}
        Vehicle:
          w: ${clw}
          beta: ${clbeta}
        Pedestrian:
          w: ${clw}
          beta: ${clbeta}
        Cyclist:
          w: ${clw}
          beta: ${clbeta}
    weight_reg:
      class_name_cfgs:
        Street:
          norm_type: 2.0
          w: 1.0e-6
        Distant:
          norm_type: 2.0
          w: 1.0e-6

  #---------- Optimization and shedulers
  enable_grad_scaler: true

  num_iters: ${num_iters}
  scheduler:
    #---------- exponential
    type: exponential
    total_steps: ${training.num_iters}
    warmup_steps: ${warmup_steps}
    decay_target_factor: ${min_factor}
    #---------- cosine
    # type: warmup_cosine
    # total_steps: ${training.num_iters}
    # warmup_steps: ${warmup_steps}
    # decay_target_factor: ${min_factor}
    #---------- milestone
    # type: multistep
    # milestones: [20000, 30000]
    # gamma: 0.33

  #---------- Logging and validation
  i_val: 500      # unit: iters
  i_backup: -1 # unit: iters
  i_save: 900     # unit: seconds
  i_log: 20
  log_grad: false
  log_param: false

  ckpt_file: null
  ckpt_ignore_keys: []
  ckpt_only_use_keys: null